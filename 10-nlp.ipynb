{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef2111f-a2d5-4ebb-8d4a-dac04bdbef12",
   "metadata": {},
   "source": [
    "# Text Analysis (NLP)\n",
    "\n",
    "Taken from [http://openonlinecourses.com/causalanalysis/TextAnalysis.asp](http://openonlinecourses.com/causalanalysis/TextAnalysis.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420a94a-7a7c-48e6-86a3-c6f97a3703f1",
   "metadata": {},
   "source": [
    "## Crawl the directory of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba427423-252f-4915-bea3-a549dd259598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T22:49:00.108481Z",
     "iopub.status.busy": "2022-04-18T22:49:00.108481Z",
     "iopub.status.idle": "2022-04-18T22:49:00.116484Z",
     "shell.execute_reply": "2022-04-18T22:49:00.116484Z",
     "shell.execute_reply.started": "2022-04-18T22:49:00.108481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "csv_files = list(pathlib.Path('./CSV.Sentiment').glob('*.csv'))\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c92455-69df-45f8-96d3-5d830c4f12f6",
   "metadata": {},
   "source": [
    "## Build maps\n",
    "\n",
    "These maps will help us map back and forth between the files and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9685681-0b7f-4e55-b57f-e16c0cd0ac44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T22:49:01.515364Z",
     "iopub.status.busy": "2022-04-18T22:49:01.515364Z",
     "iopub.status.idle": "2022-04-18T22:49:02.261713Z",
     "shell.execute_reply": "2022-04-18T22:49:02.261713Z",
     "shell.execute_reply.started": "2022-04-18T22:49:01.515364Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_file_name(file_path):\n",
    "    stem = file_path.stem\n",
    "    stem = stem.replace('+', ' ')\n",
    "    stem = stem.replace('_', '')\n",
    "    stem = stem.replace('.', '')\n",
    "    stem = stem.lower()\n",
    "    stem = stem.strip()\n",
    "    return stem\n",
    "\n",
    "# file-to-id\n",
    "f2i = {str(p): i for i, p in enumerate(csv_files)}\n",
    "\n",
    "# id-to-file\n",
    "i2f = {v: k for k, v in f2i.items()}\n",
    "\n",
    "# file-to-sentence\n",
    "f2s = {f2i[str(p)]: clean_file_name(p) for p in csv_files}\n",
    "\n",
    "# file-to-data\n",
    "f2d = {f2i[str(p)]: pd.read_csv(p)[['comment', 'classification']].assign(comment=lambda d: d['comment'].str.lower()) \n",
    "       for p in csv_files}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e554e-0c43-40ca-baf7-cabf473b202d",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Let's create vector space models `VSMs` for each one of these corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83b506bb-ad9a-4051-8428-93deccf1f2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T22:58:37.415794Z",
     "iopub.status.busy": "2022-04-18T22:58:37.415794Z",
     "iopub.status.idle": "2022-04-18T22:58:47.669090Z",
     "shell.execute_reply": "2022-04-18T22:58:47.669090Z",
     "shell.execute_reply.started": "2022-04-18T22:58:37.415794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def get_vsm(df, vectorizer_type='count'):\n",
    "    text = df['comment']\n",
    "    \n",
    "    if 'count' == vectorizer_type:\n",
    "        vectorizer = CountVectorizer(max_features=100)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(max_features=100)\n",
    "    \n",
    "    vectorizer.fit(text)\n",
    "    X = vectorizer.transform(text).todense()\n",
    "\n",
    "    count_df = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "    count_df['__y'] = df['classification']\n",
    "    \n",
    "    return {'data': count_df, 'vectorizer': vectorizer}\n",
    "\n",
    "count_vsm = {k: get_vsm(df, vectorizer_type='count') for k, df in f2d.items()}\n",
    "tfidf_vsm = {k: get_vsm(df, vectorizer_type='tfidf') for k, df in f2d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25e818-3653-4cc6-a3bf-155c3bf4b82c",
   "metadata": {},
   "source": [
    "## Learn models\n",
    "\n",
    "Let's learn a classification model (e.g. Logistic Regression) for each of the VSMs types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9065bf1-c5d9-4178-93ee-fc67d2572e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T22:58:47.671090Z",
     "iopub.status.busy": "2022-04-18T22:58:47.670090Z",
     "iopub.status.idle": "2022-04-18T22:59:01.637241Z",
     "shell.execute_reply": "2022-04-18T22:59:01.637241Z",
     "shell.execute_reply.started": "2022-04-18T22:58:47.670090Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_model(df):\n",
    "    X = df[[c for c in df.columns if c != '__y']]\n",
    "    y = np.ravel(df['__y'])\n",
    "    \n",
    "    model = LogisticRegression(random_state=37, n_jobs=-1, solver='saga', max_iter=5_000)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "count_models = {k: get_model(v['data']) for k, v in count_vsm.items()}\n",
    "tfidf_models = {k: get_model(v['data']) for k, v in tfidf_vsm.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f14b9c18-3f44-4b65-98fa-4652a5af09dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T23:03:05.999325Z",
     "iopub.status.busy": "2022-04-18T23:03:05.999325Z",
     "iopub.status.idle": "2022-04-18T23:03:06.018328Z",
     "shell.execute_reply": "2022-04-18T23:03:06.018328Z",
     "shell.execute_reply.started": "2022-04-18T23:03:05.999325Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a very uneventful experience when it came down to pain\n",
      "Index(['about', 'after', 'all', 'always', 'am', 'amazing', 'an', 'and', 'any',\n",
      "       'are',\n",
      "       ...\n",
      "       'went', 'were', 'what', 'when', 'with', 'wonderful', 'would', 'years',\n",
      "       'you', '__y'],\n",
      "      dtype='object', length=101)\n",
      "a very uneventful experience when it came down to pain [[0.84691516 0.15308484]] [[0.65470883 0.34529117]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeev\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jeev\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\jeev\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jeev\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def do_predict(fid):\n",
    "    s = f2s[fid]\n",
    "    print(s)\n",
    "    \n",
    "    count_v = count_vsm[fid]['vectorizer']\n",
    "    tfidf_v = tfidf_vsm[fid]['vectorizer']\n",
    "    \n",
    "    count_s = count_v.transform([s]).todense()\n",
    "    tfidf_s = tfidf_v.transform([s]).todense()\n",
    "    \n",
    "    count_m = count_models[fid]\n",
    "    tfidf_m = tfidf_models[fid]\n",
    "    \n",
    "    count_c = count_vsm[fid]['data'].columns\n",
    "    tfidf_c = tfidf_vsm[fid]['data'].columns\n",
    "    \n",
    "    print(count_c)\n",
    "    \n",
    "    count_p = count_m.predict_proba(count_s)\n",
    "    tfidf_p = tfidf_m.predict_proba(tfidf_s)\n",
    "    \n",
    "    print(s, count_p, tfidf_p)\n",
    "    \n",
    "do_predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8587b8-2114-4143-92c3-abf8062659fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
